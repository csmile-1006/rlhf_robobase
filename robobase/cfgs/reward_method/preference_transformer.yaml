# @package _global_
reward_method:
  _target_: robobase.reward_method.preference_transformer.PreferenceTransformer
  device: ${device}
  lr: 1e-4
  weight_decay: 1e-4
  num_train_steps: ${num_pretrain_steps}
  adaptive_lr: false
  # This would expect language embeddings using openai-clip
  # TODO: add an environment wrapper for an extra observation with language embeddings
  # But this would be environment-specific
  use_lang_cond: False
  num_label: 1
  compute_batch_size: ${batch_size}
  seq_len: ${reward_replay.seq_len}
  use_augmentation: True

  encoder_model:
    _target_: robobase.models.encoder.DINOv2Encoder
    _partial_: true
    input_shape: ???

  view_fusion_model:
    _target_: robobase.models.FusionMultiCamFeature
    _partial_: true
    input_shape: ???
    mode: flatten

  reward_model:
    _target_: robobase.models.preference_transformer.transformer.MultiViewTransformerDecoderPT
    _partial_: true
    input_shape: ???
    state_dim: ???
    action_dim: ???
    hidden_dim: 256
    enc_layers: 4
    dec_layers: 1
    dim_feedforward: 1024
    dropout: 0.1
    nheads: 4
    pre_norm: False
    use_lang_cond: ${reward_method.use_lang_cond}
    seq_len: ${reward_method.seq_len}
    position_embedding: "sine"
    num_label: ${reward_method.num_label}
